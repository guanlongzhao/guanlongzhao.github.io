<!DOCTYPE html>
<html>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171609755-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171609755-1');
</script>
<title>Guanlong Zhao, Ph.D. 赵冠龙</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="w3.css">
<link rel="stylesheet" href="style.css">
<link rel='stylesheet' href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400;1,700&display=swap">
<link rel="icon" href="media/image/favicon.png">
<body class="w3-light-grey">

<!-- Page Container -->
<div class="w3-content w3-margin-top" style="max-width:1400px;">

  <!-- The Grid -->
    <div class="w3-row-padding">

        <!-- The Main Body -->
        <div class="w3-container">
            <h2>Guanlong Zhao, Ph.D. 赵冠龙</h2>
        </div>

        <div class="w3-container">
            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>About</h3>
                <p>I am a speech researcher and engineer. My research interests broadly focus on the intersection of speech modification and speech perception. I am also interested in speech synthesis (e.g., TTS) and speech recognition (acoustic 
                    modeling in particular). Specifically, I spent five years working on topics in accent and voice conversion during my Ph.D. training. Recently, I am working on "reference-free" accent conversion and multi-speaker voice
                    conversion systems. I am trying to solve those problems with non-parallel corpora in low-resource settings.</p>
                <p>Outside of work, I like to go on a hike with friends (or alone) on a sunny day. I am also fascinated by ancient mythologies; my favorites are those from ancient Egypt and China. I have read the 
                    <i>Classic of Mountains and Seas</i> (山海经 in Chinese; 4th century BC) many times. I am grateful for the continuity of the Chinese civilization that made it possible for me to read (and understand) the extraordinary archaic 
                    literature from my ancestors.</p>
                <p>If you would like to reach out to me, my email address is <i>my first and last name put together at gmail dot com.</i></p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                    <h3>Education</h3>
                    <p>Ph.D. in Computer Science, <a href="https://www.tamu.edu/" target="_blank">Texas A&M University</a>, 2020</p>
                    <ul>
                        <li>Dissertation: Foreign accent conversion with neural acoustic modeling</li>
                        <li>Advisor: <a href="https://engineering.tamu.edu/cse/profiles/rgutierrez-osuna.html" target="_blank">Dr. Ricardo Gutierrez-Osuna</a></li>
                    </ul>
                    <p>B.S. in Applied Physics (minor in Computer Science), <a href="http://en.ustc.edu.cn/" target="_blank">University of Science and Technology of China</a>, 2015</p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Work Experience</h3>
                <p>Software Engineering Intern @ Google (Geo Machine Perception), May–August 2019</p>
                <ul>
                    <li>Built an unsupervised semantic segmentation model for large (multi-TB) Google Street View 3D Lidar point-cloud data using a combination of the <a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank">DeepLab</a> model, 2D-to-3D projection, and dense CRF</li>
                    <li>Constructed and deployed a Lidar data labeling tool into production to collect ground-truth semantic annotations</li>
                    <li>Obtained 18% relative improvements in segmentation accuracy compared with the previous internal system</li>
                </ul>
                <p>Software Engineering Intern @ Google (Speech), June–August 2018</p>
                <ul>
                    <li>Implemented a GMM forced-aligner that can generate graphemic forced-alignment for low-resource languages</li>
                    <li>Established an end-to-end pipeline to train alignment-based graphemic acoustic models for several Indic languages</li>
                    <li>Improved the load-balancing strategy of the production acoustic-model-refreshing infrastructure</li>
                </ul>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Publications</h3>
                <p>Under Review</p>
                <ol start="1">
                    <li><b>G. Zhao</b>, S. Ding, and R. Gutierrez-Osuna, "Converting foreign accent speech without a reference," submitted to <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>.</li>
                    <li>S. Ding, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Improving the speaker identity of non-parallel many-to-many voice conversion with adversarial speaker recognition," submitted to <i>Interspeech</i>, 2020.</li>
                    <li>A. Das, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Understanding the effect of voice quality and accent on talker similarity," submitted to <i>Interspeech</i>, 2020.</li>
                    <li>A. Silpachai, I. Lučić Rehman, T. A. Barriuso, J. Levis, <b>G. Zhao</b>, E. Chukharev-Khudilaynen, and R. Gutierrez-Osuna, "The effect of voice type and task on L2 learners' awareness of pronunciation errors," submitted 
                        to <i>Language Awareness</i>.</li>
                </ol>
                <p>Journal Articles</p>
                <ol start="5">
                    <li>I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "<a href="media/publication/lucic2020arabic.pdf" target="_blank">The English pronunciation of Arabic speakers: A data-driven approach to 
                        segmental error identification</a>," <i>Language Teaching Research</i>, 2020.</li>
                    <li><b>G. Zhao</b> and R. Gutierrez-Osuna, "<a href="media/publication/zhao2019taslp.pdf" target="_blank">Using phonetic posteriorgram based frame pairing for segmental accent conversion</a>," <i>IEEE/ACM Transactions on Audio, 
                        Speech, and Language Processing</i>, vol. 27, no. 10, pp. 1649–1660, 2019. <a class="block" href="https://github.com/guanlongzhao/ppg-gmm" target="_blank">code</a> <a class="block" 
                        href="https://guanlongzhao.github.io/demo/ppg-gmm/" target="_blank">demo</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, C. Liberatore, and R. Gutierrez-Osuna, "<a href="media/publication/ding2019taslp.pdf" target="_blank">Learning structured sparse representations for voice conversion</a>," <i>IEEE/ACM Transactions 
                        on Audio, Speech, and Language Processing</i>, vol. 28, pp. 343–354, 2019. <a class="block" href="https://shaojinding.github.io/samples/cssr/cssr_demo" target="_blank">demo</a></li>
                    <li>S. Ding, C. Liberatore, S. Sonsaat, I. Lučić Rehman, A. Silpachai, <b>G. Zhao</b>, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "<a href="media/publication/ding2019gsb.pdf" target="_blank">Golden speaker 
                        builder–An interactive tool for pronunciation training</a>," <i>Speech Communication</i>, vol. 115, pp. 51–66, 2019. <a class="block" href="https://goldenspeaker.engl.iastate.edu/speech/" target="_blank">demo</a></li>
                </ol>
                <p>Conference Proceedings</p>
                <ol start="9">
                    <li><b>G. Zhao</b>, S. Ding, and R. Gutierrez-Osuna, "<a href="media/publication/zhao2019interspeech.pdf" target="_blank">Foreign accent conversion by synthesizing speech from phonetic posteriorgrams</a>," 
                        in <i>Interspeech</i>, 2019, pp. 2843–2847. <a class="block" href="https://github.com/guanlongzhao/fac-via-ppg" target="_blank">code</a> 
                        <a class="block" href="https://guanlongzhao.github.io/demo/fac-via-ppg/" target="_blank">demo</a> <a class="block" href="media/publication/zhao2019interspeech_slides.pdf" target="_blank">slides</a></li>
                    <li><b>G. Zhao</b>, S. Sonsaat, A. Silpachai, I. Lučić Rehman, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "<a href="media/publication/zhao2018interspeech.pdf" target="_blank">L2-ARCTIC: A non-native English 
                        speech corpus</a>," in <i>Interspeech</i>, 2018, pp. 2783–2787. 
                        <a class="block" href="https://psi.engr.tamu.edu/l2-arctic-corpus/" target="_blank">data</a> <a class="block" href="https://github.com/guanlongzhao/kaldi-gop" target="_blank">code</a> 
                        <a class="block" href="media/publication/zhao2018interspeech_slides.pdf" target="_blank">slides</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, C. Liberatore, and R. Gutierrez-Osuna, "<a href="media/publication/ding2018interspeech.pdf" target="_blank">Improving sparse representations in exemplar-based voice conversion with a 
                        phoneme-selective objective function</a>," in <i>Interspeech</i>, 2018, pp. 476–480.</li>
                    <li>C. Liberatore, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "<a href="media/publication/liberatore2018icassp.pdf" target="_blank">Voice conversion through residual warping in a sparse, anchor-based representation of speech</a>," 
                        in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 2018, pp. 5284–5288.</li>
                    <li><b>G. Zhao</b>, S. Sonsaat, J. Levis, E. Chukharev-Hudilainen, and R. Gutierrez-Osuna, "<a href="media/publication/zhao2018icassp.pdf" target="_blank">Accent conversion using phonetic posteriorgrams</a>," 
                        in <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2018, pp. 5314–5318. <a class="block" href="https://github.com/guanlongzhao/ppg-gmm" target="_blank">code</a> 
                        <a class="block" href="https://guanlongzhao.github.io/demo/icassp18/" target="_blank">demo</a> <a class="block" href="media/publication/zhao2018icassp_poster.pdf" target="_blank">poster</a></li>
                    <li>G. Angello, A. B. Manam, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "<a href="media/publication/angello2018haptics.pdf" target="_blank">Training behavior of successful tacton-phoneme learners</a>," 
                        in <i>IEEE Haptics Symposium (WIP)</i>, 2018.</li>
                    <li><b>G. Zhao</b> and R. Gutierrez-Osuna, "<a href="media/publication/zhao2017icassp.pdf" target="_blank">Exemplar selection methods in voice conversion</a>," in <i>IEEE International Conference on Acoustics, Speech, and 
                        Signal Processing (ICASSP)</i>, 2017, pp. 5525–5529. <a class="block" href="https://guanlongzhao.github.io/demo/icassp17/" target="_blank">demo</a> 
                        <a class="block" href="media/publication/zhao2017icassp_poster.pdf" target="_blank">poster</a></li>
                </ol>
                <p>Book Chapter</p>
                <ol start="16">
                    <li>Y. Liu, <b>G. Zhao</b>, B. Gong, Y. Li, R. Raj, N. Goel, S. Kesav, S. Gottimukkala, Z. Wang, W. Ren, and D. Tao, "<a href="https://www.sciencedirect.com/science/article/pii/B9780128136591000196" target="_blank">Chapter 10 — Image 
                        dehazing: Improved techniques</a>," in <i>Deep Learning through Sparse and Low-Rank Modeling</i>: Elsevier, 2019, pp. 251–262. 
                        <a class="block" href="https://github.com/TAMU-VITA/dehaze" target="_blank">code</a></li>
                </ol>
                <p>Preprints</p>
                <ol start="17">
                    <li>Y. Liu, <b>G. Zhao</b>, B. Gong, Y. Li, R. Raj, N. Goel, S. Kesav, S. Gottimukkala, Z. Wang, W. Ren, and D. Tao, "<a href="media/publication/liu2018dehaze2.pdf" target="_blank">Improved techniques for learning to dehaze and 
                        beyond: A collective study</a>," <i>arXiv preprint arXiv:1807.00202</i>, 2018. <a class="block" href="https://github.com/TAMU-VITA/dehaze" target="_blank">code</a></li>
                    <li>Y. Liu and <b>G. Zhao</b>, "<a href="media/publication/liu2018dehaze1.pdf" target="_blank">PAD-Net: A perception-aided single image dehazing network</a>," <i>arXiv preprint arXiv:1805.03146</i>, 2018. 
                        <a class="block" href="https://github.com/guanlongzhao/single-image-dehazing" target="_blank">code</a></li>
                </ol>
                <p>Abstracts</p>
                <ol start="19">
                    <li>I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "<a href="media/publication/lucic2019l2prw.pdf" target="_blank">Pronunciation errors — A systematic approach to diagnosis</a>," 
                    in <i>L2 Pronunciation Research Workshop: Bridging the Gap between Research and Practice</i>, 2019, pp. 23–24.</li>
                    <li>S. Sonsaat, E. Chukharev-Hudilainen, I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, S. Ding, C. Liberatore, and R. Gutierrez-Osuna, "<a href="media/publication/sonsaat2019epip6.pdf" target="_blank">Golden Speaker Builder, an interactive tool for 
                        pronunciation training: User studies</a>," in <i>6th International Conference on English Pronunciation: Issues & Practices (EPIP6)</i>, 2019, p. 72.</li>
                    <li>S. Ding, C. Liberatore, <b>G. Zhao</b>, S. Sonsaat, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "<a href="media/publication/ding2017gsb.pdf" target="_blank">Golden Speaker Builder: an interactive online 
                        tool for L2 learners to build pronunciation models</a>," in <i>Pronunciation in Second Language Learning and Teaching (PSLLT)</i>, 2017, pp. 25–26.</li>
                </ol>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Professional Service</h3>
                <p>Reviewer for:</p>
                <ul>
                    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank">IEEE Transactions on Image Processing</a></li>
                    <li><a href="https://www.lltjournal.org/" target="_blank">Language Learning & Technology</a> (secondary reviewer)</li>
                    <li>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP): <a href="https://2020.ieeeicassp.org/" target="_blank">2020</a> (secondary reviewer)</li>
                    <li>International Conference on Advances in Signal, Image and Video Processing (SIGNAL): <a href="https://www.iaria.org/conferences2020/SIGNAL20.html" target="_blank">2020</a> (Technical Program Committee member)</li>
                    <li>Annual Conference of the International Speech Communication Association (Interspeech): <a href="https://interspeech2019.org/" target="_blank">2019</a> (secondary reviewer)</li>
                </ul>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Teaching</h3>
                <p>Teaching Assistant: <a href="http://courses.cs.tamu.edu/rgutier/csce482_s16/index.htm" target="_blank">CSCE 482: Senior Capstone Design (Spring 2016)</a></p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Honors</h3>
                <ul>
                    <li>Graduate Student Travel Award (for Interspeech'19), Department of Computer Science and Engineering, Texas A&M University, 2019</li>
                    <li>Graduate Student Presentation Grant (for ICASSP'17), Office of Graduate and Professional Studies, Texas A&M University, 2017</li>
                    <li>Outstanding Graduate Award, University of Science and Technology of China, 2015</li>
                    <li>Outstanding Undergraduate Student Scholarship, University of Science and Technology of China, 2011–2014</li>
                    <li>Second Prize @ Chinese Chemistry Olympiad (Provincial Level), Chinese Chemical Society, 2010</li>
                </ul>
            </div>

        <!-- End Main Body -->
        </div>

    <!-- End Grid -->
    </div>
  
  <!-- End Page Container -->
</div>

<footer class="w3-container w3-center">
    <p><a href="https://guanlongzhao.github.io/" target="_blank">Personal Website</a> | <a href="calendar.html" target="_blank">Calendar</a> | <a href="https://www.linkedin.com/in/guanlongzhao/" target="_blank">LinkedIn</a> | 
        <a href="https://github.com/guanlongzhao" target="_blank">GitHub</a> | <a href="https://scholar.google.com/citations?user=IZhdKR8AAAAJ&hl=en" target="_blank">Google Scholar</a> | 
        <a href="https://psi.engr.tamu.edu/people/guanlong-zhao/" target="_blank">PSI Lab</a></p>
    <p>Copyright <script>document.write(new Date().getFullYear())</script> Guanlong Zhao</p>
</footer>

</body>
</html>
