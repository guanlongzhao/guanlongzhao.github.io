<!DOCTYPE html>
<html lang="en">
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171609755-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171609755-1');
</script>
<title>Guanlong Zhao, Ph.D. 赵冠龙</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="w3.css">
<link rel="stylesheet" href="style.css">
<link rel='stylesheet' href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400;1,700&display=swap">
<link rel="icon" href="media/image/favicon.png">
</head>

<body class="w3-light-grey">

<!-- Page Container -->
<div class="w3-content w3-margin-top" style="max-width:1400px;">

  <!-- The Grid -->
    <div class="w3-row-padding">

        <!-- The Main Body -->
        <div class="w3-container">
            <h2>Guanlong Zhao, Ph.D. <span lang="zh-Hans">赵冠龙</span></h2>
        </div>

        <div class="w3-container">
            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>About</h3>
                <p>I am a speech researcher and engineer. My research interests broadly focus on the intersection of speech modification and speech perception.
                    I am also interested in speech synthesis (e.g., TTS) and speech recognition (E2E models in particular).
                    Specifically, I spent five years working on topics in accent and voice conversion during my Ph.D. training.</p>
                <p>If you would like to reach out to me, my email address is <u><i>zhao at aggienetwork dot com</i></u>. How do I pronounce my name? In PinYin, it is written as Guàn-Lóng Zhào; the tones 
                    are fourth, second, and fourth. Mapping to American English phonemes, it roughly sounds like Guan-Loan Chao. &#127752 Cheers!</p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                    <h3>Education</h3>
                    <p>Ph.D. in Computer Science, <a href="https://www.tamu.edu/" target="_blank">Texas A&M University</a>, 2020</p>
                    <ul>
                        <li>Dissertation: <a href="https://hdl.handle.net/1969.1/192349" target="_blank">Foreign accent conversion with neural acoustic modeling</a></li>
                        <li>Advisor: <a href="https://engineering.tamu.edu/cse/profiles/rgutierrez-osuna.html" target="_blank">Dr. Ricardo Gutierrez-Osuna</a></li>
                    </ul>
                    <p>B.S. in Applied Physics (minor in Computer Science), <a href="https://en.ustc.edu.cn/" target="_blank">University of Science and Technology of China</a>, 2015</p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Work Experience</h3>
                <p>Software Engineer @ Google (Speech), August 2020–Present</p>
                <p>Research and Teaching Assistant @ Texas A&M University (Department of Computer Science and Engineering), September 2015–May 2020</p>
                <!-- Company name in brand colors -->
                <!-- <span style="color: #4285F4">G</span><span style="color: #EA4335">o</span><span style="color: #FBBC04">o</span><span style="color: #4285F4">g</span><span style="color: #34A853">l</span><span style="color: #EA4335">e</span> -->
                <p>Software Engineering Intern @ Google (Geo Machine Perception), May–August 2019</p>
                <p>Software Engineering Intern @ Google (Speech), June–August 2018</p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Publications</h3>
                <!-- <p>Under Review</p>
                <ol start="1"></ol> -->
                <p>Journal Articles</p>
                <ol start="1">
                    <li>S. Ding, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Accentron: Foreign accent conversion to arbitrary non-native speakers using zero-shot learning,"
                        <i>Computer Speech & Language</i>, vol. 72, 2022. 
                        <a class="block" href="media/publication/ding2021csl.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://shaojinding.github.io/samples/accentron/" target="_blank">demo</a></li>
                    <li><b>G. Zhao</b>, S. Ding, and R. Gutierrez-Osuna, "Converting foreign accent speech without a reference," <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>, vol. 29, pp. 2367–2381, 2021.
                        <a class="block" href="media/publication/zhao2021taslp.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://guanlongzhao.github.io/demo/reference-free-ac/" target="_blank">demo</a></li>
                    <li>I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "The English pronunciation of Arabic speakers: A data-driven approach to segmental error identification," <i>Language Teaching Research</i>, 
                        2020. <a class="block" href="media/publication/lucic2020arabic.pdf" target="_blank">pdf</a> <a class="block" href="media/publication/lucic2020arabic_summary.pdf" target="_blank">summary</a></li>
                    <li><b>G. Zhao</b> and R. Gutierrez-Osuna, "Using phonetic posteriorgram based frame pairing for segmental accent conversion," <i>IEEE/ACM Transactions on Audio, 
                        Speech, and Language Processing</i>, vol. 27, no. 10, pp. 1649–1660, 2019. <a class="block" href="media/publication/zhao2019taslp.pdf" target="_blank">pdf</a> <a class="block" href="https://github.com/guanlongzhao/ppg-gmm" target="_blank">code</a> <a class="block" 
                        href="https://guanlongzhao.github.io/demo/ppg-gmm/" target="_blank">demo</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, C. Liberatore, and R. Gutierrez-Osuna, "Learning structured sparse representations for voice conversion," <i>IEEE/ACM Transactions 
                        on Audio, Speech, and Language Processing</i>, vol. 28, pp. 343–354, 2019. <a class="block" href="media/publication/ding2019taslp.pdf" target="_blank">pdf</a> <a class="block" href="https://shaojinding.github.io/samples/cssr/cssr_demo" target="_blank">demo</a></li>
                    <li>S. Ding, C. Liberatore, S. Sonsaat, I. Lučić Rehman, A. Silpachai, <b>G. Zhao</b>, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "Golden speaker 
                        builder–An interactive tool for pronunciation training," <i>Speech Communication</i>, vol. 115, pp. 51–66, 2019. <a class="block" href="media/publication/ding2019gsb.pdf" target="_blank">pdf</a> 
                        <a class="block" href="https://github.com/shaojinding/Golden-Speaker-Builder" target="_blank">code</a>
                        <a class="block" href="https://goldenspeaker.engl.iastate.edu/speech/" target="_blank">demo</a></li>
                </ol>
                <p>Conference Proceedings</p>
                <ol start="7">
                    <li><b>G. Zhao</b>, Q. Wang, H. Lu, Y. Huang, and I. Lopez Moreno, "Augmenting transformer-transducer based speaker change detection with token-level training loss,"
                        in <i>IEEE International Conference on Acoustics, Speech, and 
                            Signal Processing (ICASSP)</i>, 2023. <a class="block" href="media/publication/zhao2022augmenting.pdf" target="_blank">pdf</a>
                            <a class="block" href="media/publication/zhao2023icassp_poster.pdf" target="_blank">poster</a>
                        <a class="block" href="https://github.com/google/speaker-id/tree/master/publications/ScdLoss" target="_blank">resources</a></li>
                    <li>B. Labrador<sup>*</sup>, <b>G. Zhao</b><sup>*</sup>, I. Lopez Moreno<sup>*</sup>, A. Scorza Scarpati, L. Fowl, and Quan Wang, "Exploring sequence-to-sequence transformer-transducer models for keyword spotting,"
                        in <i>IEEE International Conference on Acoustics, Speech, and 
                            Signal Processing (ICASSP)</i>, 2023. <sup>*</sup>Equal contribution. <a class="block" href="media/publication/labrador2022exploring.pdf" target="_blank">pdf</a></li>
                    <li>A. Hair, <b>G. Zhao</b>, B. Ahmed, K. Ballard, and R. Gutierrez-Osuna, "Assessing posterior-based mispronunciation detection on field-collected recordings from child speech therapy sessions,"
                        in <i>Interspeech</i>, 2021. pp. 2936–2940. <a class="block" href="media/publication/hair2021interspeech.pdf" target="_blank">pdf</a></li>
                    <li>A. Silpachai, I. Lučić Rehman, T. A. Barriuso, J. Levis, E. Chukharev-Khudilaynen, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Effects of voice type and task on L2 learners' awareness of pronunciation errors,"
                        in <i>Interspeech</i>, 2021. pp. 1952–1956. <a class="block" href="media/publication/silpachai2021interspeech.pdf" target="_blank">pdf</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Improving the speaker identity of non-parallel many-to-many voice conversion with adversarial speaker recognition," in <i>Interspeech</i>, 2020. pp. 776–780.
                        <a class="block" href="media/publication/ding2020interspeech.pdf" target="_blank">pdf</a> <a class="block" href="https://github.com/shaojinding/Adversarial-Many-to-Many-VC" target="_blank">code</a> 
                        <a class="block" href="https://shaojinding.github.io/samples/adv/" target="_blank">demo</a> 
                        <a class="block" href="https://interspeech2020.oss-cn-beijing.aliyuncs.com/Mon/Mon-2-7-2.mp4" target="_blank">video</a></li>
                    <li>A. Das, <b>G. Zhao</b>, J. Levis, E. Chukharev-Hudilainen, and R. Gutierrez-Osuna, "Understanding the effect of voice quality and accent on talker similarity," in <i>Interspeech</i>, 2020. pp. 1763–1767.
                        <a class="block" href="media/publication/das2020interspeech.pdf" target="_blank">pdf</a> 
                        <a class="block" href="https://interspeech2020.oss-cn-beijing.aliyuncs.com/Tue/Tue-1-7-9.mp4" target="_blank">video</a></li>
                    <li><b>G. Zhao</b>, S. Ding, and R. Gutierrez-Osuna, "Foreign accent conversion by synthesizing speech from phonetic posteriorgrams," 
                        in <i>Interspeech</i>, 2019, pp. 2843–2847. <a class="block" href="media/publication/zhao2019interspeech.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/guanlongzhao/fac-via-ppg" target="_blank">code</a> 
                        <a class="block" href="https://guanlongzhao.github.io/demo/fac-via-ppg/" target="_blank">demo</a> <a class="block" href="media/publication/zhao2019interspeech_slides.pdf" target="_blank">slides</a></li>
                    <li><b>G. Zhao</b>, S. Sonsaat, A. Silpachai, I. Lučić Rehman, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "L2-ARCTIC: A non-native English 
                        speech corpus," in <i>Interspeech</i>, 2018, pp. 2783–2787. <a class="block" href="media/publication/zhao2018interspeech.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://psi.engr.tamu.edu/l2-arctic-corpus/" target="_blank">data</a> <a class="block" href="https://github.com/guanlongzhao/kaldi-gop" target="_blank">code</a> 
                        <a class="block" href="media/publication/zhao2018interspeech_slides.pdf" target="_blank">slides</a></li>
                    <li>S. Ding, <b>G. Zhao</b>, C. Liberatore, and R. Gutierrez-Osuna, "Improving sparse representations in exemplar-based voice conversion with a 
                        phoneme-selective objective function," in <i>Interspeech</i>, 2018, pp. 476–480. <a class="block" href="media/publication/ding2018interspeech.pdf" target="_blank">pdf</a></li>
                    <li>C. Liberatore, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Voice conversion through residual warping in a sparse, anchor-based representation of speech," 
                        in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, 2018, pp. 5284–5288. <a class="block" href="media/publication/liberatore2018icassp.pdf" target="_blank">pdf</a>
                        <a class="block" href="media/publication/liberatore2018icassp_poster.pdf" target="_blank">poster</a></li>
                    <li><b>G. Zhao</b>, S. Sonsaat, J. Levis, E. Chukharev-Hudilainen, and R. Gutierrez-Osuna, "Accent conversion using phonetic posteriorgrams," 
                        in <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2018, pp. 5314–5318. <a class="block" href="media/publication/zhao2018icassp.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/guanlongzhao/ppg-gmm" target="_blank">code</a> 
                        <a class="block" href="https://guanlongzhao.github.io/demo/icassp18/" target="_blank">demo</a> <a class="block" href="media/publication/zhao2018icassp_poster.pdf" target="_blank">poster</a></li>
                    <li>G. Angello, A. B. Manam, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Training behavior of successful tacton-phoneme learners," 
                        in <i>IEEE Haptics Symposium (WIP)</i>, 2018. <a class="block" href="media/publication/angello2018haptics.pdf" target="_blank">pdf</a></li>
                    <li><b>G. Zhao</b> and R. Gutierrez-Osuna, "Exemplar selection methods in voice conversion," in <i>IEEE International Conference on Acoustics, Speech, and 
                        Signal Processing (ICASSP)</i>, 2017, pp. 5525–5529. <a class="block" href="media/publication/zhao2017icassp.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://guanlongzhao.github.io/demo/icassp17/" target="_blank">demo</a> 
                        <a class="block" href="media/publication/zhao2017icassp_poster.pdf" target="_blank">poster</a></li>
                </ol>
                <p>Book Chapter</p>
                <ol start="20">
                    <li>Y. Liu, <b>G. Zhao</b>, B. Gong, Y. Li, R. Raj, N. Goel, S. Kesav, S. Gottimukkala, Z. Wang, W. Ren, and D. Tao, "Chapter 10 — Image dehazing: Improved techniques,"
                        in <i>Deep Learning through Sparse and Low-Rank Modeling</i>: Elsevier, 2019, pp. 251–262.
                        <a class="block" href="https://www.sciencedirect.com/science/article/pii/B9780128136591000196" target="_blank">link</a>
                        <a class="block" href="https://github.com/TAMU-VITA/dehaze" target="_blank">code</a></li>
                </ol>
                <p>Preprints</p>
                <ol start="21">
                    <li>Q. Wang, Y. Huang, H. Lu, <b>G. Zhao</b>, and I. Lopez Moreno, "Highly efficient real-time streaming and fully on-device speaker diarization with multi-stage clustering,"
                        <i>arXiv preprint arXiv:2210.13690</i>, 2022. <a class="block" href="media/publication/wang2022highly.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/wq2012/SpectralCluster" target="_blank">code</a></li>
                    <li>Y. Liu, <b>G. Zhao</b>, B. Gong, Y. Li, R. Raj, N. Goel, S. Kesav, S. Gottimukkala, Z. Wang, W. Ren, and D. Tao, "Improved techniques for learning to dehaze and 
                        beyond: A collective study," <i>arXiv preprint arXiv:1807.00202</i>, 2018. <a class="block" href="media/publication/liu2018dehaze2.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/TAMU-VITA/dehaze" target="_blank">code</a></li>
                    <li>Y. Liu and <b>G. Zhao</b>, "PAD-Net: A perception-aided single image dehazing network," <i>arXiv preprint arXiv:1805.03146</i>, 2018. 
                        <a class="block" href="media/publication/liu2018dehaze1.pdf" target="_blank">pdf</a>
                        <a class="block" href="https://github.com/guanlongzhao/single-image-dehazing" target="_blank">code</a></li>
                    <li>A. Datta, <b>G. Zhao</b>, B. Ramabhadran, E. Weinstein, "LSTM acoustic models learn to align and pronounce with graphemes," <i>arXiv preprint arXiv:2008.06121</i>, 2020.
                        (Work done as an intern at Google NYC during summer 2018.)
                        <a class="block" href="media/publication/datta2020arxiv.pdf" target="_blank">pdf</a></li>
                </ol>
                <p>Abstracts</p>
                <ol start="25">
                    <li>I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, and R. Gutierrez-Osuna, "Pronunciation errors — A systematic approach to diagnosis," 
                    in <i>L2 Pronunciation Research Workshop: Bridging the Gap between Research and Practice</i>, 2019, pp. 23–24. <a class="block" href="media/publication/lucic2019l2prw.pdf" target="_blank">pdf</a></li>
                    <li>S. Sonsaat, E. Chukharev-Hudilainen, I. Lučić Rehman, A. Silpachai, J. Levis, <b>G. Zhao</b>, S. Ding, C. Liberatore, and R. Gutierrez-Osuna, "Golden Speaker Builder, an interactive tool for pronunciation training: User 
                        studies," in <i>6th International Conference on English Pronunciation: Issues & Practices (EPIP6)</i>, 2019, p. 72. <a class="block" href="media/publication/sonsaat2019epip6.pdf" target="_blank">pdf</a></li>
                    <li>S. Ding, C. Liberatore, <b>G. Zhao</b>, S. Sonsaat, E. Chukharev-Hudilainen, J. Levis, and R. Gutierrez-Osuna, "Golden Speaker Builder: an interactive online tool for L2 learners to build pronunciation models,"
                        in <i>Pronunciation in Second Language Learning and Teaching (PSLLT)</i>, 2017, pp. 25–26. <a class="block" href="media/publication/ding2017gsb.pdf" target="_blank">pdf</a></li>
                </ol>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Professional Service</h3>
                <p>Reviewer for:</p>
                <ul>
                    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank">IEEE Transactions on Image Processing</a></li>
                    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206" target="_blank">IEEE Transactions on Information Forensics and Security</a></li>
                    <li><a href="https://www.cell.com/heliyon/home" target="_blank">Heliyon</a></li>
                    <li><a href="https://www.lltjournal.org/" target="_blank">Language Learning & Technology</a></li>
                    <li><a href="https://www.sciencedirect.com/journal/computer-speech-and-language" target="_blank">Computer Speech & Language</a></li>
                    <li><a href="https://www.aimspress.com/journal/MBE" target="_blank">Mathematical Biosciences and Engineering</a></li>
                    <li><a href="https://www.aimspress.com/journal/era" target="_blank">Electronic Research Archive</a></li>
                    <li>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP): <a href="https://2020.ieeeicassp.org/" target="_blank">2020</a>, <a href="https://2022.ieeeicassp.org/" target="_blank">2022</a>, <a href="https://2023.ieeeicassp.org/" target="_blank">2023</a> (<a href="media/others/outstanding_reviewer_icassp23.pdf" target="_blank">Outstanding Reviewer</a>)</li>
                    <li>International Conference on Advances in Signal, Image and Video Processing (SIGNAL): <a href="https://www.iaria.org/conferences2020/SIGNAL20.html" target="_blank">2020</a>, <a href="https://www.iaria.org/conferences2021/SIGNAL21.html" target="_blank">2021</a>, <a href="https://www.iaria.org/conferences2022/ComSIGNAL22.html" target="_blank">2022</a>, <a href="https://www.iaria.org/conferences2023/ComSIGNAL23.html" target="_blank">2023</a></li>
                    <li>Annual Conference of the International Speech Communication Association (Interspeech): <a href="https://interspeech2019.org/" target="_blank">2019</a>, <a href="https://interspeech2021.org/" target="_blank">2021</a>,
                        <a href="https://interspeech2022.org/" target="_blank">2022</a>, <a href="https://interspeech2023.org/" target="_blank">2023</a></li>
                    <li>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU): <a href="https://asru2021.signalprocessingsociety.org/asru2021.org/index.html" target="_blank">2021</a>, <a href="http://www.asru2023.org/" target="_blank">2023</a></li>
                    <li>IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA): <a href="https://signalprocessingsociety.org/blog/waspaa-2023-2023-ieee-workshop-applications-signal-processing-audio-and-acoustics" target="_blank">2023</a></li>
                </ul>
                <p>Students mentored:</p>
                <ul>
                    <li><a href="https://ana-kuznetsova.github.io/" target="_blank">Anastasia Kuznetsova</a>, Research Intern @ Google, 2023</li>
                    <li><a href="https://scholar.google.com/citations?hl=en&user=biA-c9sAAAAJ" target="_blank">Beltrán Labrador</a>, Research Intern @ Google, 2022, 2023</li>
                </ul>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Teaching</h3>
                <p>Teaching Assistant: <a href="https://people.engr.tamu.edu/rgutier/web_courses/csce482_s16/contact.htm" target="_blank">CSCE 482: Senior Capstone Design (Spring 2016)</a></p>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>Honors</h3>
                <ul>
                    <li>Graduate Student Travel Award (for Interspeech'19), Department of Computer Science and Engineering, Texas A&M University, 2019</li>
                    <li>Graduate Student Presentation Grant (for ICASSP'17), Office of Graduate and Professional Studies, Texas A&M University, 2017</li>
                    <li>Outstanding Graduate Award, University of Science and Technology of China, 2015</li>
                    <li>Outstanding Undergraduate Student Scholarship, University of Science and Technology of China, 2011–2014</li>
                    <li>Second Prize @ Chinese Chemistry Olympiad (Provincial Level), Chinese Chemical Society, 2010</li>
                </ul>
            </div>

            <div class="w3-container w3-card w3-white w3-margin-bottom">
                <h3>L2-ARCTIC Corpus</h3>
                <p>The <a href="https://psi.engr.tamu.edu/l2-arctic-corpus/" target="_blank">L2-ARCTIC corpus</a> is a multi-purpose non-native English speech dataset. I took a leading role in this project, where I designed the data collection 
                schemes and the annotation standards. I also spent a lot of time manually cleaning the raw speech recordings and performing quality control to ensure that the speech data and annotations were consistent and high-quality. The 
                recordings were collected at Iowa State University (ISU), led by <a href="https://faculty.sites.iastate.edu/jlevis/" target="_blank">Dr. John Levis</a> and his students in the Department of English. The annotations were mostly 
                done by <a href="http://alifsilpachai.com/" target="_blank">Dr. Alif Silpachai</a> and <a href="https://www.ivanarehman.com/" target="_blank">Dr. Ivana Lučić Rehman</a>.
                The project spanned around two years. We released the first version at Interspeech 2018, and we continued to add more data to the corpus. Its most recent version is almost 2.4x the size of the initial 
                release.</p>
                <p>We initially designed the corpus for the accent conversion task, and that was why we chose to use the CMU-ARCTIC prompts in the first place. Along the way, we were also working on some projects related to mispronunciation
                    detection (MPD) and realized that there were limited open-source resources for MPD. We noticed that many of the CMU-ARCTIC sentences were hard for the participants to speak, which, on the one hand, made the recording
                    sessions difficult, on the other hand, elicited rich pronunciation errors in non-native speech productions. As a result, we decided to annotate part of the corpus for phonetic errors. All the sentences we annotated were carefully
                    selected by Dr. Levis to reflect the pronunciation issues that might happen given the speakers' native languages.
                </p>
                <p>I use this corpus in all my publications on accent conversion. I found it is well-suited for the task because it allows me to test the algorithms on speakers with different accents, fluency level, age, and gender. I also use 
                    this corpus for MPD research. To the best of my knowledge, this is probably the largest open-source annotated MPD corpus. If you are interested in using the corpus for your projects, you can find access guidelines on
                    its <a href="https://psi.engr.tamu.edu/l2-arctic-corpus/" target="_blank">official project site</a>. I would be happy to see it being used in more projects. If you have any questions regarding downloading/using the corpus, please
                    feel free to drop me an email.
                </p>
            </div>

        <!-- End Main Body -->
        </div>

    <!-- End Grid -->
    </div>
  
  <!-- End Page Container -->
</div>

<footer class="w3-container w3-center">
    <p><a href="https://guanlongzhao.github.io/" target="_blank">Personal Website</a> | <a href="https://www.linkedin.com/in/guanlongzhao/" target="_blank">LinkedIn</a> | 
        <a href="https://github.com/guanlongzhao" target="_blank">GitHub</a> | <a href="https://scholar.google.com/citations?user=IZhdKR8AAAAJ&hl=en" target="_blank">Google Scholar</a> | 
        <a href="https://psi.engr.tamu.edu/people/guanlong-zhao/" target="_blank">PSI Lab</a></p>
    <p>Copyright &copy <script>document.write(new Date().getFullYear())</script> Guanlong Zhao. All Rights Reserved.</p>
</footer>

</body>
</html>
